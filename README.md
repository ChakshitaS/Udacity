# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset represents Bank marketing campaign of a bank. The data indicates the potential customer likely for the deposit of the bank represented by the column ("y").We use Classification method to determine the result.

We will use two machine learning experiments, with primary metric as "Accuracy".

1) Scikit-learn  with Logistic Regression Classifier, Script Run config as an estimator using the Sklearn env and hyperdrive run for hyperparameter tuning.

2) AutoML experiment by selection of classification task and their hyper parameters.

Best models selected under each of the pipelines are as follows:

i) Scikit-learn: Logistic Regression model with 90.08% Accuracy and with parameter values: C: uniform(0.05, 2), max_iter:  choice(range(10,200,20)).

ii) Azure AutoML: VotingEnsemble classifier with 91.679 % Accuracy.
Hence Azure Automl is the best experiment to run to achieve maximum accuracy.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
1) To initiate any experiment to run , we have to spin a compute for faster execution of the pipeline. In this experiment, we have used computecluster with a VM size of STANDARD_D2_V2 have max_nodes set to 4. 
2) Run the python script to make sure all the defined parameters have been validated with the dataset set to tabular format and splitting the data to train and test datasets.
Note: If the indentation is not followed in the pythin script. The script will throw an exception while running any experiment.
3) Create a conda dependencies file if there are any particular versions/packages of azure ml/python to be used.
4The Hyperparameter tuning requires the following parameters to be defined:
 i)Parameter Sampler: Random Paramter sampling technique was used.
 ii)Estimator: We have define a run script config which defines the SKlearn estimator with the environment defined.
 iii)Policy: Bandit Policy has been used as a early stopping policy
**What are the benefits of the parameter sampler you chose?**
Random Parameter Sampling uses random values for unbiased selection. In this method, we have defined "--C": uniform(0.05, 2), "--max_iter": choice(range(10,200,20))  as the parameter for continuous and discrete selection of values for our dataset. It Supports and improvises the search for better results with the parameters defined.
**What are the benefits of the early stopping policy you chose?**
The configuation used for Bandit Policy is  **BanditPolicy(evaluation_interval=2, slack_factor=0.1, slack_amount=None, delay_evaluation=0)**
This policy will terminate if the primary metric is not defined within the slackfactor/amount in this case the slack_factor is 0.1 which will yield the best model.

Lastly, we use Logistic regression as our classification type for our best model  to get the best accuracy score for the hyperdrive run experiment.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
